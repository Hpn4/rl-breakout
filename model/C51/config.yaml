train:
  num_steps: 100000000

  batch_size: 64
  gamma: 0.99
  lr: 2.5e-4 #6.25e-5

  eps_start: 1.0
  eps_end: 0.01
  eps_decay_frames: 1000000
  replay_start: 50000 # Min size of the replay mem before starting to optimize
  clip_gradient: 10.0

  target_update_freq: 10000
  optimize_freq: 4 # Optimze every N steps. In fact every optimize_freq * frame_skip steps

model:
  save_dir: model/C51
  double: False # https://arxiv.org/pdf/1509.06461
  dueling: False
  
  C51: True
  n_atoms: 101
  v_min: -2
  v_max: 900

env:
  mod_name: model/C51
  memory_capacity: 300000
  game: BreakoutNoFrameskip-v4 #ALE/Breakout-v5

  repeat_action_probability: 0.0
  num_stack: 4
  noop_max: 30
  seed: 42

  terminal_on_life_loss: False
  episodic_life: True # When loosing a life, notify the agent of end of episode without reset the env
  clip_reward: False # Clips in range [-1, 1]. By default breakout have [1, 3].
  
  looping_penalty: True # Set reward to -1 if it does not break a tile after 200 actions
  lose_life_penalty: False # Set reward to -1 when losing a life

  grayscale_obs: True
  screen_size: 84
  scale_obs: True
  frame_skip: 4

eval:
  q_eval_states_count: 100 # Number of random states to compute Q
  avg_window: 100 # Size of the window for averaging R and Q
  eval_freq: 10000 # Every 10k steps
  ep_per_eval: 10
