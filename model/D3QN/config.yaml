train:
  num_steps: 100000000

  batch_size: 32
  gamma: 0.99
  lr: 6.25e-5

  eps_start: 1.0
  eps_end: 0.05
  eps_decay_frames: 1000000
  replay_start: 50000 # Min size of the replay mem before starting to optimize
  clip_gradient: 0.5

  target_update_freq: 10000
  optimize_freq: 4 # Optimze every N steps. In fact every optimize_freq * frame_skip steps

model:
  save_dir: model/D3QN
  double: True # https://arxiv.org/pdf/1509.06461

env:
  mod_name: model/D3QN
  memory_capacity: 300000
  game: BreakoutNoFrameskip-v4 #ALE/Breakout-v5

  repeat_action_probability: 0.0
  num_stack: 4
  noop_max: 30
  seed: 42

  terminal_on_life_loss: False
  episodic_life: True # When loosing a life, notify the agent of end of episode without reset the env
  clip_reward: True # Clips in range [-1, 1]. By default breakout have [1, 3].
  
  grayscale_obs: True
  screen_size: 84
  scale_obs: True
  frame_skip: 4

eval:
  q_eval_states_count: 100 # Number of random states to compute Q
  avg_window: 100 # Size of the window for averaging R and Q
  eval_freq: 200 # x5 if episodic_life.
  ep_per_eval: 10
